{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-01T14:42:27.202281Z",
     "iopub.status.busy": "2022-05-01T14:42:27.201911Z",
     "iopub.status.idle": "2022-05-01T14:42:27.209701Z",
     "shell.execute_reply": "2022-05-01T14:42:27.208761Z",
     "shell.execute_reply.started": "2022-05-01T14:42:27.202241Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:43:34.318466Z",
     "iopub.status.busy": "2022-05-01T14:43:34.317970Z",
     "iopub.status.idle": "2022-05-01T14:43:35.062464Z",
     "shell.execute_reply": "2022-05-01T14:43:35.061455Z",
     "shell.execute_reply.started": "2022-05-01T14:43:34.318420Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", sep = \"\\t\")\n",
    "#train2 = pd.read_csv(\"../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\", \"\\t\")\n",
    "print(train.head())\n",
    "#the shape is 25000 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:43:35.214025Z",
     "iopub.status.busy": "2022-05-01T14:43:35.213711Z",
     "iopub.status.idle": "2022-05-01T14:43:36.197305Z",
     "shell.execute_reply": "2022-05-01T14:43:36.196564Z",
     "shell.execute_reply.started": "2022-05-01T14:43:35.213986Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv.zip\",sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:43:36.285933Z",
     "iopub.status.busy": "2022-05-01T14:43:36.285481Z",
     "iopub.status.idle": "2022-05-01T14:43:36.295977Z",
     "shell.execute_reply": "2022-05-01T14:43:36.295239Z",
     "shell.execute_reply.started": "2022-05-01T14:43:36.285885Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train.loc[0][\"review\"])\n",
    "#有许多html符号, 要将他去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:43:36.396575Z",
     "iopub.status.busy": "2022-05-01T14:43:36.396269Z",
     "iopub.status.idle": "2022-05-01T14:43:37.000220Z",
     "shell.execute_reply": "2022-05-01T14:43:36.999084Z",
     "shell.execute_reply.started": "2022-05-01T14:43:36.396541Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#\"html.parse\" 是固定参数,不要忘记写\n",
    "example = BeautifulSoup(train.loc[0][\"review\"], \"html.parser\")\n",
    "print(example.get_text())\n",
    "import re\n",
    "p = re.compile(r'[^a-zA-A]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:43:37.001968Z",
     "iopub.status.busy": "2022-05-01T14:43:37.001679Z",
     "iopub.status.idle": "2022-05-01T14:43:37.009851Z",
     "shell.execute_reply": "2022-05-01T14:43:37.008625Z",
     "shell.execute_reply.started": "2022-05-01T14:43:37.001930Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(p))\n",
    "#re.patern\n",
    "print(type(p.sub(\" \", example.get_text())))\n",
    "letters_only = re.sub('[^A-Za-z]', ' ', example.get_text())\n",
    "print(type(letters_only))\n",
    "#str\n",
    "words = letters_only.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:43:37.011726Z",
     "iopub.status.busy": "2022-05-01T14:43:37.011484Z",
     "iopub.status.idle": "2022-05-01T14:43:46.039887Z",
     "shell.execute_reply": "2022-05-01T14:43:46.038927Z",
     "shell.execute_reply.started": "2022-05-01T14:43:37.011693Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile(r'[^a-zA-A]')\n",
    "reviews = []\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops = set( stopwords.words('english'));\n",
    "#print(stops)\n",
    "\n",
    "for i in train.loc[:][\"review\"] :\n",
    "    #去掉html,只取文本\n",
    "    finished_row_word = []\n",
    "    sub_html = BeautifulSoup(i, \"html.parser\").get_text();\n",
    "    #符号替换为空格\n",
    "    sub_space_html = p.sub(\" \", sub_html);\n",
    "    #print(pure)\n",
    "    pure_words = sub_space_html.lower().split();\n",
    "\n",
    "    for word in pure_words:\n",
    "        if word not in stops:\n",
    "            #print(word)\n",
    "            finished_row_word.append(word);\n",
    "            \n",
    "            \n",
    "    #语法：  'sep'.join(seq)\n",
    "    #参数说明\n",
    "    #sep：分隔符。可以为空\n",
    "    #seq：要连接的元素序列、字符串、元组、字典\n",
    "    #上面的语法即：以sep作为分隔符，将seq所有的元素合并成一个新的字符串\n",
    "    finished_str = \" \".join(finished_row_word)\n",
    "    reviews.append(finished_str)\n",
    "            \n",
    "    \n",
    "print(\"clean the reviews success\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:43:46.042399Z",
     "iopub.status.busy": "2022-05-01T14:43:46.041980Z",
     "iopub.status.idle": "2022-05-01T14:43:53.845151Z",
     "shell.execute_reply": "2022-05-01T14:43:53.844191Z",
     "shell.execute_reply.started": "2022-05-01T14:43:46.042359Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_reviews = []\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops = set( stopwords.words('english'));\n",
    "#print(stops)\n",
    "\n",
    "for i in test.loc[:][\"review\"] :\n",
    "    #去掉html,只取文本\n",
    "    finished_row_word = []\n",
    "    sub_html = BeautifulSoup(i, \"html.parser\").get_text();\n",
    "    #符号替换为空格\n",
    "    sub_space_html = p.sub(\" \", sub_html);\n",
    "    #print(pure)\n",
    "    pure_words = sub_space_html.lower().split();\n",
    "\n",
    "    for word in pure_words:\n",
    "        if word not in stops:\n",
    "            #print(word)\n",
    "            finished_row_word.append(word);\n",
    "            \n",
    "            \n",
    "    #语法：  'sep'.join(seq)\n",
    "    #参数说明\n",
    "    #sep：分隔符。可以为空\n",
    "    #seq：要连接的元素序列、字符串、元组、字典\n",
    "    #上面的语法即：以sep作为分隔符，将seq所有的元素合并成一个新的字符串\n",
    "    finished_str = \" \".join(finished_row_word)\n",
    "    test_reviews.append(finished_str)\n",
    "            \n",
    "    \n",
    "print(\"clean the test reviews success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:43:53.847388Z",
     "iopub.status.busy": "2022-05-01T14:43:53.846818Z",
     "iopub.status.idle": "2022-05-01T14:43:54.285433Z",
     "shell.execute_reply": "2022-05-01T14:43:54.284515Z",
     "shell.execute_reply.started": "2022-05-01T14:43:53.847340Z"
    }
   },
   "outputs": [],
   "source": [
    "a = pd.Series(reviews)\n",
    "a.head()\n",
    "a.to_csv(\"reviews.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:49:51.719291Z",
     "iopub.status.busy": "2022-05-01T14:49:51.718077Z",
     "iopub.status.idle": "2022-05-01T14:49:58.743057Z",
     "shell.execute_reply": "2022-05-01T14:49:58.742260Z",
     "shell.execute_reply.started": "2022-05-01T14:49:51.719237Z"
    }
   },
   "outputs": [],
   "source": [
    "# 从sklearn.feature_extraction.text里导入CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)\n",
    "train_data_features = vectorizer.fit_transform(reviews)\n",
    "train_data_features = pd.DataFrame(train_data_features.toarray())\n",
    "\n",
    "test_data_features = vectorizer.transform(test_reviews)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T14:51:08.619902Z",
     "iopub.status.busy": "2022-05-01T14:51:08.619554Z",
     "iopub.status.idle": "2022-05-01T14:52:29.528188Z",
     "shell.execute_reply": "2022-05-01T14:52:29.527339Z",
     "shell.execute_reply.started": "2022-05-01T14:51:08.619869Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #集成学习中的随机森林\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "forest = forest.fit( train_data_features, train[\"sentiment\"] )\n",
    "\n",
    "result = forest.predict(test_data_features)\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"submission.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-01T14:44:14.097633Z",
     "iopub.status.idle": "2022-05-01T14:44:14.098505Z",
     "shell.execute_reply": "2022-05-01T14:44:14.098327Z",
     "shell.execute_reply.started": "2022-05-01T14:44:14.098305Z"
    }
   },
   "outputs": [],
   "source": [
    "#Tokenizer用来对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示。\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#使用一系列文档来生成token词典，texts为list类，每个元素为一个文档。\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "#将多个文档转换为word下标的向量形式,shape为[len(texts)，len(text)] -- (文档数，每条文档的长度)\n",
    "#texts_to_sequences(texts) \n",
    "\n",
    "#将多个文档转换为矩阵表示,shape为[len(texts),num_words]\n",
    "tokenized_train = tokenizer.texts_to_sequences(reviews)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_features = 6000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "\n",
    "\n",
    "#tokenized_train[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-01T14:44:14.099503Z",
     "iopub.status.idle": "2022-05-01T14:44:14.099781Z",
     "shell.execute_reply": "2022-05-01T14:44:14.099653Z",
     "shell.execute_reply.started": "2022-05-01T14:44:14.099638Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "X_train = pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-01T14:44:14.101038Z",
     "iopub.status.idle": "2022-05-01T14:44:14.101390Z",
     "shell.execute_reply": "2022-05-01T14:44:14.101200Z",
     "shell.execute_reply.started": "2022-05-01T14:44:14.101184Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dedict = defaultdict(int)\n",
    "print(dedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-01T14:44:14.102264Z",
     "iopub.status.idle": "2022-05-01T14:44:14.102603Z",
     "shell.execute_reply": "2022-05-01T14:44:14.102467Z",
     "shell.execute_reply.started": "2022-05-01T14:44:14.102450Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "inp = Input(shape=(maxlen, ))\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(60, return_sequences=True, name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "print(\"successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-01T14:44:14.103293Z",
     "iopub.status.idle": "2022-05-01T14:44:14.103575Z",
     "shell.execute_reply": "2022-05-01T14:44:14.103446Z",
     "shell.execute_reply.started": "2022-05-01T14:44:14.103431Z"
    }
   },
   "outputs": [],
   "source": [
    "train['sentiment'] = to_categorical(train['sentiment'], num_classes=2)\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "model.fit(X_train, train['sentiment'], batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
